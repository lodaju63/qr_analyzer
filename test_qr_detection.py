"""
QR ì½”ë“œ íƒì§€ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
YOLO ëª¨ë¸ì˜ QR ì½”ë“œ íƒì§€ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import cv2
import os
import sys
import numpy as np
from pathlib import Path
import time
import json
from datetime import datetime

# YOLO ëª¨ë¸ import
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("âš ï¸ ultralyticsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install ultralyticsë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    sys.exit(1)

# PIL import (í•œê¸€ í°íŠ¸ ì§€ì›ìš©)
try:
    from PIL import Image, ImageDraw, ImageFont
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False


def put_korean_text(img, text, position, font_size=20, color=(0, 255, 0)):
    """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
    if not PIL_AVAILABLE:
        cv2.putText(img, text, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
        return img
    
    try:
        font_path = 'data/font/NanumGothic.ttf'
        if os.path.exists(font_path):
            font = ImageFont.truetype(font_path, font_size)
        else:
            font = ImageFont.load_default()
    except:
        font = ImageFont.load_default()
    
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color)
    img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
    return img_cv


def calculate_iou(bbox1, bbox2):
    """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ì˜ IoU(Intersection over Union) ê³„ì‚°"""
    x1_1, y1_1, x2_1, y2_1 = bbox1
    x1_2, y1_2, x2_2, y2_2 = bbox2
    
    # êµì§‘í•© ì˜ì—­ ê³„ì‚°
    x1_i = max(x1_1, x1_2)
    y1_i = max(y1_1, y1_2)
    x2_i = min(x2_1, x2_2)
    y2_i = min(y2_1, y2_2)
    
    # êµì§‘í•©ì´ ì—†ëŠ” ê²½ìš°
    if x2_i <= x1_i or y2_i <= y1_i:
        return 0.0
    
    intersection = (x2_i - x1_i) * (y2_i - y1_i)
    
    # ê° ë°•ìŠ¤ì˜ ë©´ì 
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
    # í•©ì§‘í•© ì˜ì—­
    union = area1 + area2 - intersection
    
    # IoU ê³„ì‚°
    iou = intersection / union if union > 0 else 0.0
    return iou


def filter_overlapping_detections(detections, iou_threshold=0.5):
    """
    ê²¹ì¹˜ëŠ” íƒì§€ ê²°ê³¼ ì œê±° (NMS - Non-Maximum Suppression)
    
    Args:
        detections: [{'bbox': [x1, y1, x2, y2], 'confidence': float}, ...]
        iou_threshold: ê²¹ì¹¨ ì„ê³„ê°’ (0.5 = 50% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
    
    Returns:
        filtered_detections: í•„í„°ë§ëœ íƒì§€ ê²°ê³¼
    """
    if not detections:
        return []
    
    # ì‹ ë¢°ë„(confidence) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒì´ ìš°ì„ )
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
    filtered = []
    for det in detections:
        is_overlapping = False
        bbox1 = det['bbox']
        
        for filtered_det in filtered:
            bbox2 = filtered_det['bbox']
            iou = calculate_iou(bbox1, bbox2)
            
            if iou > iou_threshold:
                is_overlapping = True
                break
        
        if not is_overlapping:
            filtered.append(det)
    
    return filtered


def detect_qr_with_yolo(model, frame, conf_threshold=0.25, iou_threshold=0.5):
    """
    YOLO ëª¨ë¸ë¡œ QR ì½”ë“œ ìœ„ì¹˜ íƒì§€
    
    Args:
        model: YOLO ëª¨ë¸
        frame: ì…ë ¥ í”„ë ˆì„ (BGR)
        conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.25)
        iou_threshold: ê²¹ì¹¨ ì„ê³„ê°’ (ê¸°ë³¸: 0.5)
    
    Returns:
        detections: [{'bbox': [x1, y1, x2, y2], 'confidence': float}, ...]
    """
    detections = []
    
    try:
        # YOLO íƒì§€
        results = model(frame, conf=conf_threshold, verbose=False, imgsz=640)
        result = results[0]
        
        if result.boxes is not None and len(result.boxes) > 0:
            for box in result.boxes:
                conf = float(box.conf[0])
                xyxy = box.xyxy[0].cpu().numpy()
                x1, y1, x2, y2 = map(int, xyxy)
                
                # íŒ¨ë”© ì¶”ê°€ (QR ì½”ë“œ ê²½ê³„ í™•ë³´)
                pad = 20
                h, w = frame.shape[:2]
                x1 = max(0, x1 - pad)
                y1 = max(0, y1 - pad)
                x2 = min(w, x2 + pad)
                y2 = min(h, y2 + pad)
                
                detections.append({
                    'bbox': [x1, y1, x2, y2],
                    'confidence': conf
                })
        
        # Overlap threshold ì ìš© (NMS)
        detections = filter_overlapping_detections(detections, iou_threshold=iou_threshold)
    
    except Exception as e:
        print(f"âš ï¸ íƒì§€ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    return detections


def test_single_image(model, image_path, output_dir="test_results", conf_threshold=0.25, iou_threshold=0.5, save_result=True):
    """ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ QR ì½”ë“œ íƒì§€ í…ŒìŠ¤íŠ¸"""
    
    # ì´ë¯¸ì§€ ì½ê¸°
    if not os.path.exists(image_path):
        print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None
    
    frame = cv2.imread(image_path)
    if frame is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None
    
    h, w = frame.shape[:2]
    
    # QR ì½”ë“œ íƒì§€
    start_time = time.time()
    detections = detect_qr_with_yolo(model, frame, conf_threshold, iou_threshold)
    detect_time = time.time() - start_time
    
    # ê²°ê³¼ ì‹œê°í™”
    result_frame = frame.copy()
    
    for i, det in enumerate(detections):
        x1, y1, x2, y2 = det['bbox']
        conf = det['confidence']
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
        cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # ì‹ ë¢°ë„ í‘œì‹œ
        text = f"QR{i+1}: {conf:.2f}"
        text_pos = (x1, y1 - 10) if y1 > 20 else (x1, y2 + 20)
        result_frame = put_korean_text(result_frame, text, text_pos, font_size=16, color=(0, 255, 0))
    
    # ì •ë³´ í‘œì‹œ
    info_text = f"Detections: {len(detections)} | Time: {detect_time*1000:.1f}ms | Conf: {conf_threshold} | IoU: {iou_threshold}"
    result_frame = put_korean_text(result_frame, info_text, (10, 30), font_size=16, color=(255, 255, 255))
    
    # ê²°ê³¼ ì €ì¥
    if save_result:
        os.makedirs(output_dir, exist_ok=True)
        image_name = Path(image_path).stem
        output_path = os.path.join(output_dir, f"{image_name}_detected.jpg")
        cv2.imwrite(output_path, result_frame)
    
    # ê²°ê³¼ ì •ë³´
    result_info = {
        'image_path': image_path,
        'image_size': f"{w}x{h}",
        'detections': len(detections),
        'detect_time_ms': detect_time * 1000,
        'conf_threshold': conf_threshold,
        'iou_threshold': iou_threshold,
        'detection_details': [
            {
                'id': i+1,
                'bbox': det['bbox'],
                'confidence': float(det['confidence'])
            }
            for i, det in enumerate(detections)
        ]
    }
    
    return result_info


def test_image_batch(model, image_dir, output_dir="test_results", conf_threshold=0.25, iou_threshold=0.5):
    """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì¼ê´„ í…ŒìŠ¤íŠ¸"""
    
    if not os.path.exists(image_dir):
        print(f"âŒ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        return
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    image_files = []
    for ext in image_extensions:
        image_files.extend(Path(image_dir).glob(f'*{ext}'))
        image_files.extend(Path(image_dir).glob(f'*{ext.upper()}'))
    
    if not image_files:
        print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        return
    
    image_files = sorted(image_files)
    total_images = len(image_files)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“· ì¼ê´„ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"{'='*60}")
    print(f"   ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {image_dir}")
    print(f"   ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {total_images}ê°œ")
    print(f"   Confidence Threshold: {conf_threshold}")
    print(f"   IoU Threshold: {iou_threshold}")
    print(f"{'='*60}\n")
    
    # ê²°ê³¼ ì €ì¥ìš©
    all_results = []
    total_detections = 0
    total_time = 0
    
    # ê° ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
    for idx, image_path in enumerate(image_files, 1):
        print(f"[{idx}/{total_images}] ì²˜ë¦¬ ì¤‘: {image_path.name}...", end=' ')
        
        result = test_single_image(model, str(image_path), output_dir, conf_threshold, iou_threshold, save_result=True)
        
        if result:
            all_results.append(result)
            total_detections += result['detections']
            total_time += result['detect_time_ms']
            print(f"âœ… {result['detections']}ê°œ íƒì§€ ({result['detect_time_ms']:.1f}ms)")
        else:
            print("âŒ ì‹¤íŒ¨")
    
    # í†µê³„ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µê³„")
    print(f"{'='*60}")
    print(f"   ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
    print(f"   ì´ íƒì§€ ê°œìˆ˜: {total_detections}ê°œ")
    print(f"   í‰ê·  íƒì§€ ê°œìˆ˜: {total_detections/total_images:.2f}ê°œ/ì´ë¯¸ì§€")
    print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_time/1000:.2f}ì´ˆ")
    print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/total_images:.1f}ms/ì´ë¯¸ì§€")
    
    # íƒì§€ ì„±ê³µë¥ 
    detected_images = sum(1 for r in all_results if r['detections'] > 0)
    detection_rate = (detected_images / total_images * 100) if total_images > 0 else 0
    print(f"   íƒì§€ ì„±ê³µ ì´ë¯¸ì§€: {detected_images}ê°œ ({detection_rate:.1f}%)")
    
    # Confidence í†µê³„
    all_confidences = []
    for result in all_results:
        for det in result['detection_details']:
            all_confidences.append(det['confidence'])
    
    if all_confidences:
        print(f"   í‰ê·  Confidence: {np.mean(all_confidences):.3f}")
        print(f"   ìµœì†Œ Confidence: {np.min(all_confidences):.3f}")
        print(f"   ìµœëŒ€ Confidence: {np.max(all_confidences):.3f}")
    
    print(f"{'='*60}")
    
    # JSON ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    json_path = os.path.join(output_dir, f"test_results_{timestamp}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump({
            'test_info': {
                'model_path': str(model.ckpt_path) if hasattr(model, 'ckpt_path') else 'unknown',
                'image_dir': image_dir,
                'total_images': total_images,
                'conf_threshold': conf_threshold,
                'iou_threshold': iou_threshold,
                'timestamp': timestamp
            },
            'summary': {
                'total_detections': total_detections,
                'avg_detections_per_image': total_detections/total_images if total_images > 0 else 0,
                'total_time_sec': total_time/1000,
                'avg_time_ms': total_time/total_images if total_images > 0 else 0,
                'detected_images': detected_images,
                'detection_rate_percent': detection_rate,
                'avg_confidence': float(np.mean(all_confidences)) if all_confidences else 0,
                'min_confidence': float(np.min(all_confidences)) if all_confidences else 0,
                'max_confidence': float(np.max(all_confidences)) if all_confidences else 0
            },
            'results': all_results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {json_path}")
    print(f"ğŸ’¾ ì´ë¯¸ì§€ ê²°ê³¼ ì €ì¥: {output_dir}/")


def test_video(model, video_path, output_dir="test_results", conf_threshold=0.25, iou_threshold=0.5, max_frames=None, show_display=True):
    """ë¹„ë””ì˜¤ì—ì„œ QR ì½”ë“œ íƒì§€ í…ŒìŠ¤íŠ¸ (í™”ë©´ í‘œì‹œ í¬í•¨)"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ¬ ë¹„ë””ì˜¤ íƒì§€ í…ŒìŠ¤íŠ¸: {video_path}")
    print(f"{'='*60}")
    
    # ë¹„ë””ì˜¤ ì—´ê¸°
    if not os.path.exists(video_path):
        print(f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    # ë¹„ë””ì˜¤ ì •ë³´
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # FPSê°€ 0ì´ê±°ë‚˜ ì´ìƒí•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    if fps <= 0 or fps > 120:
        fps = 30.0
        print(f"   âš ï¸ FPS ì •ë³´ê°€ ì—†ê±°ë‚˜ ì´ìƒí•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ 30 FPS ì‚¬ìš©")
    
    print(f"   í•´ìƒë„: {width}x{height}")
    print(f"   FPS: {fps:.2f}")
    print(f"   ì´ í”„ë ˆì„: {total_frames}")
    print(f"   Confidence Threshold: {conf_threshold}")
    print(f"   IoU Threshold: {iou_threshold}")
    print(f"   í™”ë©´ í‘œì‹œ: {'ON' if show_display else 'OFF'}")
    if max_frames:
        print(f"   ìµœëŒ€ ì²˜ë¦¬ í”„ë ˆì„: {max_frames}")
    print(f"{'='*60}\n")
    
    # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    os.makedirs(output_dir, exist_ok=True)
    video_name = Path(video_path).stem
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(output_dir, f"{video_name}_detected_{timestamp}.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    print(f"ğŸ’¾ ì¶œë ¥ ë¹„ë””ì˜¤: {output_path}")
    
    # í™”ë©´ í‘œì‹œìš© í•´ìƒë„ ì¡°ì •
    display_width = 1280
    display_height = 720
    if width > display_width:
        scale = display_width / width
        display_height = int(height * scale)
    
    # í†µê³„
    frame_count = 0
    total_detections = 0
    detected_frames = 0
    total_detect_time = 0
    max_detections_in_frame = 0
    all_confidences = []
    
    # FPS ì œì–´ë¥¼ ìœ„í•œ ë³€ìˆ˜
    frame_interval = 1.0 / fps
    paused = False
    start_time = time.time()  # ì „ì²´ ì‹œì‘ ì‹œê°„
    result_frame = None
    
    print("â–¶ï¸ íƒì§€ ì‹œì‘... (ì›ë³¸ ì†ë„ë¡œ ì¬ìƒ)")
    if show_display:
        print("   ğŸ’¡ ESC í‚¤: ì¢…ë£Œ, SPACE í‚¤: ì¼ì‹œì •ì§€/ì¬ìƒ")
    
    try:
        while True:
            if not paused:
                ret, frame = cap.read()
                if not ret:
                    print("\nğŸ“º ì˜ìƒ ì¬ìƒ ì™„ë£Œ!")
                    break
                
                frame_count += 1
                if max_frames and frame_count > max_frames:
                    break
                
                # QR ì½”ë“œ íƒì§€
                detect_start = time.time()
                detections = detect_qr_with_yolo(model, frame, conf_threshold, iou_threshold)
                detect_time = time.time() - detect_start
                total_detect_time += detect_time
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                num_detections = len(detections)
                if num_detections > 0:
                    detected_frames += 1
                    total_detections += num_detections
                    max_detections_in_frame = max(max_detections_in_frame, num_detections)
                    for det in detections:
                        all_confidences.append(det['confidence'])
                
                # ê²°ê³¼ ì‹œê°í™”
                result_frame = frame.copy()
                
                for i, det in enumerate(detections):
                    x1, y1, x2, y2 = det['bbox']
                    conf = det['confidence']
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
                    cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    
                    # ì‹ ë¢°ë„ í‘œì‹œ
                    text = f"QR{i+1}: {conf:.2f}"
                    text_pos = (x1, y1 - 10) if y1 > 20 else (x1, y2 + 20)
                    result_frame = put_korean_text(result_frame, text, text_pos, font_size=14, color=(0, 255, 0))
                
                # ì •ë³´ í‘œì‹œ
                current_fps = 1.0 / detect_time if detect_time > 0 else 0
                info_text = f"Frame: {frame_count}/{total_frames} | Detections: {num_detections} | FPS: {current_fps:.1f}"
                result_frame = put_korean_text(result_frame, info_text, (10, 30), font_size=16, color=(255, 255, 255))
                
                # ë¹„ë””ì˜¤ ì €ì¥ (ì›ë³¸ í•´ìƒë„ë¡œ)
                out.write(result_frame)
            
            # í™”ë©´ í‘œì‹œ
            if show_display and result_frame is not None:
                display_frame = cv2.resize(result_frame, (display_width, display_height))
                
                if paused:
                    pause_text = "PAUSED - Press SPACE to resume"
                    cv2.putText(display_frame, pause_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                
                cv2.imshow('QR Detection Test - Video', display_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESC í‚¤
                    print("\nğŸ›‘ ì‚¬ìš©ìê°€ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
                    break
                elif key == ord(' '):  # SPACE í‚¤
                    paused = not paused
                    if paused:
                        print("â¸ï¸  ì¼ì‹œì •ì§€")
                    else:
                        print("â–¶ï¸  ì¬ìƒ")
            
            # FPS ì œì–´ (ì›ë³¸ ì†ë„ë¡œ ì¬ìƒ)
            if not paused and frame_count > 0:
                elapsed = time.time() - start_time
                expected_time = frame_count * frame_interval
                sleep_time = expected_time - elapsed
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10í”„ë ˆì„ë§ˆë‹¤, í™”ë©´ í‘œì‹œ ì•ˆ í•  ë•Œë§Œ)
            if not show_display and frame_count % 10 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘... {frame_count}/{total_frames} í”„ë ˆì„ ({frame_count/total_frames*100:.1f}%)")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    finally:
        end_time = time.time()  # ì „ì²´ ì¢…ë£Œ ì‹œê°„
        total_elapsed_time = end_time - start_time  # ì „ì²´ ê²½ê³¼ ì‹œê°„
        cap.release()
        out.release()
        if show_display:
            cv2.destroyAllWindows()
    
    # ìµœì¢… í†µê³„
    avg_detect_time = total_detect_time / frame_count if frame_count > 0 else 0
    avg_detections = total_detections / frame_count if frame_count > 0 else 0
    detection_rate = (detected_frames / frame_count * 100) if frame_count > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"{'='*60}")
    print(f"   ì²˜ë¦¬ëœ í”„ë ˆì„: {frame_count}ê°œ")
    print(f"   íƒì§€ëœ í”„ë ˆì„: {detected_frames}ê°œ ({detection_rate:.1f}%)")
    print(f"   ì´ íƒì§€ ê°œìˆ˜: {total_detections}ê°œ")
    print(f"   í”„ë ˆì„ë‹¹ í‰ê·  íƒì§€: {avg_detections:.2f}ê°œ")
    print(f"   í”„ë ˆì„ë‹¹ ìµœëŒ€ íƒì§€: {max_detections_in_frame}ê°œ")
    print(f"   í‰ê·  íƒì§€ ì‹œê°„: {avg_detect_time*1000:.1f}ms/í”„ë ˆì„")
    print(f"   ìˆœìˆ˜ íƒì§€ ì‹œê°„: {total_detect_time:.2f}ì´ˆ (íƒì§€ë§Œ)")
    print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed_time:.2f}ì´ˆ (ì „ì²´ ê²½ê³¼ ì‹œê°„)")
    print(f"   ì›ë³¸ ë¹„ë””ì˜¤ ê¸¸ì´: {total_frames/fps:.2f}ì´ˆ")
    
    if all_confidences:
        print(f"   í‰ê·  Confidence: {np.mean(all_confidences):.3f}")
        print(f"   ìµœì†Œ Confidence: {np.min(all_confidences):.3f}")
        print(f"   ìµœëŒ€ Confidence: {np.max(all_confidences):.3f}")
    
    print(f"\nğŸ’¾ ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"{'='*60}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='QR ì½”ë“œ íƒì§€ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸')
    parser.add_argument('input_path', type=str, help='ì…ë ¥ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ íŒŒì¼ ë˜ëŠ” ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--model', type=str, default='model1.pt', help='YOLO ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: model1.pt)')
    parser.add_argument('--output', type=str, default='test_results', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: test_results)')
    parser.add_argument('--conf', type=float, default=0.25, help='ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ë³¸: 0.25)')
    parser.add_argument('--iou', type=float, default=0.5, help='ê²¹ì¹¨ ì„ê³„ê°’ (ê¸°ë³¸: 0.5)')
    parser.add_argument('--max-frames', type=int, default=None, help='ë¹„ë””ì˜¤ ìµœëŒ€ ì²˜ë¦¬ í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸: ì „ì²´)')
    parser.add_argument('--no-display', action='store_true', help='ë¹„ë””ì˜¤ í™”ë©´ í‘œì‹œ ì•ˆ í•¨ (ì§„í–‰ ìƒí™©ë§Œ ì¶œë ¥)')
    
    args = parser.parse_args()
    
    # YOLO ëª¨ë¸ ë¡œë“œ
    if not os.path.exists(args.model):
        print(f"âŒ YOLO ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        sys.exit(1)
    
    print(f"ğŸ” YOLO ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model}")
    try:
        model = YOLO(args.model)
        print("âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ì…ë ¥ ê²½ë¡œ í™•ì¸
    if not os.path.exists(args.input_path):
        print(f"âŒ ì…ë ¥ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_path}")
        sys.exit(1)
    
    # íŒŒì¼ íƒ€ì… í™•ì¸
    if os.path.isdir(args.input_path):
        # ë””ë ‰í† ë¦¬: ì¼ê´„ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
        test_image_batch(model, args.input_path, args.output, args.conf, args.iou)
    else:
        # íŒŒì¼: ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
        file_ext = Path(args.input_path).suffix.lower()
        
        if file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            # ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
            result = test_single_image(model, args.input_path, args.output, args.conf, args.iou, save_result=True)
            if result:
                print(f"\nâœ… íƒì§€ ì™„ë£Œ: {result['detections']}ê°œ QR ì½”ë“œ ë°œê²¬")
                print(f"   ì²˜ë¦¬ ì‹œê°„: {result['detect_time_ms']:.1f}ms")
        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            # ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸
            show_display = not args.no_display
            test_video(model, args.input_path, args.output, args.conf, args.iou, args.max_frames, show_display=show_display)
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}")
            print("   ì§€ì› í˜•ì‹: .jpg, .jpeg, .png, .bmp, .mp4, .avi, .mov, .mkv")
            sys.exit(1)


if __name__ == "__main__":
    main()

