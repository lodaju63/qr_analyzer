"""
l.pt ëª¨ë¸ íŒŒì¼ ë™ì‘ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
ì˜ìƒ ë° ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì§€ì›
"""

import cv2
import numpy as np
import os
from pathlib import Path
import torch
import time
import datetime
import threading
from queue import Queue, Empty

# Ultralytics YOLO ëª¨ë¸ ë¡œë“œ ì‹œë„
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("âš ï¸ ultralyticsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í‘œì‹œìš© ì„¤ì •
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

def test_model_info(model_path='l.pt'):
    """ëª¨ë¸ ì •ë³´ í™•ì¸"""
    print("=" * 60)
    print(f"ğŸ“¦ ëª¨ë¸ íŒŒì¼: {model_path}")
    print("=" * 60)
    
    if not os.path.exists(model_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    
    file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
    print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
    
    # Ultralytics YOLO ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„
    if ULTRALYTICS_AVAILABLE:
        try:
            print("\nğŸ” Ultralytics YOLO ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„...")
            model = YOLO(model_path)
            
            print(f"âœ… ëª¨ë¸ íƒ€ì…: YOLO (Ultralytics)")
            print(f"ğŸ“‹ ëª¨ë¸ ì •ë³´:")
            print(f"   - Task: {model.task if hasattr(model, 'task') else 'Unknown'}")
            print(f"   - Classes: {len(model.names) if hasattr(model, 'names') else 'Unknown'}")
            
            if hasattr(model, 'names'):
                print(f"   - í´ë˜ìŠ¤ ëª©ë¡:")
                for idx, name in model.names.items():
                    print(f"     [{idx}] {name}")
            
            if hasattr(model, 'model'):
                model_info = model.model
                print(f"   - ëª¨ë¸ êµ¬ì¡°: {type(model_info).__name__}")
            
            return model, 'yolo'
        except Exception as e:
            print(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ì¼ë°˜ PyTorch ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„
    try:
        print("\nğŸ” PyTorch ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„...")
        checkpoint = torch.load(model_path, map_location='cpu')
        
        print(f"âœ… ëª¨ë¸ íƒ€ì…: PyTorch")
        print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤:")
        for key in checkpoint.keys():
            if isinstance(checkpoint[key], dict):
                print(f"   - {key}: (dict with {len(checkpoint[key])} keys)")
                if len(checkpoint[key].keys()) < 10:
                    for subkey in checkpoint[key].keys():
                        print(f"     - {subkey}: {type(checkpoint[subkey]).__name__}")
            elif isinstance(checkpoint[key], (list, tuple)):
                print(f"   - {key}: ({type(checkpoint[key]).__name__} with {len(checkpoint[key])} items)")
            else:
                print(f"   - {key}: {type(checkpoint[key]).__name__}")
        
        return checkpoint, 'pytorch'
    except Exception as e:
        print(f"âŒ PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def test_yolo_detection(model, image_path, conf_threshold=0.25):
    """YOLO ëª¨ë¸ë¡œ ì´ë¯¸ì§€ íƒì§€ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ–¼ï¸  ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸: {os.path.basename(image_path)}")
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None
    
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {image.shape[1]}x{image.shape[0]}")
    
    # íƒì§€ ì‹¤í–‰
    try:
        results = model(image, conf=conf_threshold, verbose=False)
        result = results[0]
        
        print(f"   íƒì§€ëœ ê°ì²´ ìˆ˜: {len(result.boxes) if result.boxes is not None else 0}")
        
        detections = []
        if result.boxes is not None and len(result.boxes) > 0:
            for i, box in enumerate(result.boxes):
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                xyxy = box.xyxy[0].cpu().numpy()
                
                class_name = result.names[cls] if hasattr(result, 'names') else f"Class_{cls}"
                
                detections.append({
                    'class': class_name,
                    'confidence': conf,
                    'bbox': xyxy,
                    'class_id': cls
                })
                
                print(f"   [{i+1}] {class_name}: {conf:.2%} at [{int(xyxy[0])}, {int(xyxy[1])}, {int(xyxy[2])}, {int(xyxy[3])}]")
        
        return {
            'image': image,
            'detections': detections,
            'result': result
        }
    except Exception as e:
        print(f"âŒ íƒì§€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def visualize_results(image, detections, save_path=None):
    """íƒì§€ ê²°ê³¼ ì‹œê°í™”"""
    if detections is None or len(detections) == 0:
        print("   âš ï¸ íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì´ë¯¸ì§€ ë³µì‚¬
    vis_image = image.copy()
    
    # BGR to RGB ë³€í™˜
    vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)
    
    # Matplotlibìœ¼ë¡œ ì‹œê°í™”
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(vis_image_rgb)
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    for det in detections:
        bbox = det['bbox']
        x1, y1, x2, y2 = map(int, bbox)
        
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        rect = Rectangle((x1, y1), x2-x1, y2-y1, 
                        linewidth=2, edgecolor='red', facecolor='none')
        ax.add_patch(rect)
        
        # ë¼ë²¨ ì¶”ê°€
        label = f"{det['class']}: {det['confidence']:.2%}"
        ax.text(x1, y1-5, label, color='red', fontsize=10, 
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    ax.axis('off')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
    
    plt.show()

def test_video_detection(model, video_path, conf_threshold=0.25, 
                        frame_interval=1, show_video=True, save_output=True,
                        process_scale=1.0):
    """YOLO ëª¨ë¸ë¡œ ì˜ìƒ íƒì§€ í…ŒìŠ¤íŠ¸
    
    Args:
        model: YOLO ëª¨ë¸
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        frame_interval: íƒì§€ ê°„ê²© (1=ëª¨ë“  í”„ë ˆì„, 5=5í”„ë ˆì„ë§ˆë‹¤, 30=30í”„ë ˆì„ë§ˆë‹¤)
        show_video: í™”ë©´ í‘œì‹œ ì—¬ë¶€
        save_output: ê²°ê³¼ ì˜ìƒ ì €ì¥ ì—¬ë¶€
        process_scale: ì²˜ë¦¬ í•´ìƒë„ ìŠ¤ì¼€ì¼ (1.0=ì›ë³¸, 0.5=50%, 0.25=25%)
    
    Note:
        - ì›ë³¸ í•´ìƒë„ë¡œ íƒì§€ (í”„ë ˆì„ ë¦¬ì‚¬ì´ì§• ì—†ìŒ)
        - frame_interval=1ë¡œ ì„¤ì •í•˜ë©´ ëª¨ë“  í”„ë ˆì„ì—ì„œ íƒì§€ (ì‹¤ì‹œê°„ì²˜ëŸ¼)
    """
    print(f"\nğŸ¬ ì˜ìƒ í…ŒìŠ¤íŠ¸: {os.path.basename(video_path)}")
    print("=" * 60)
    
    # ë¹„ë””ì˜¤ íŒŒì¼ í™•ì¸
    if not os.path.exists(video_path):
        print(f"âŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return None
    
    # ë¹„ë””ì˜¤ ìº¡ì²˜
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return None
    
    # ë¹„ë””ì˜¤ ì •ë³´
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = total_frames / fps if fps > 0 else 0
    
    # ì²˜ë¦¬ í•´ìƒë„ ì„¤ì •
    process_width = int(width * process_scale)
    process_height = int(height * process_scale)
    scale_x = width / process_width if process_width > 0 else 1.0
    scale_y = height / process_height if process_height > 0 else 1.0
    
    print(f"ğŸ“¹ ì˜ìƒ ì •ë³´:")
    print(f"   ì›ë³¸ í•´ìƒë„: {width}x{height}")
    print(f"   ì²˜ë¦¬ í•´ìƒë„: {process_width}x{process_height} (ìŠ¤ì¼€ì¼: {process_scale*100:.0f}%)")
    print(f"   FPS: {fps:.2f}")
    print(f"   ì´ í”„ë ˆì„: {total_frames}")
    print(f"   ê¸¸ì´: {duration:.2f}ì´ˆ")
    print(f"   íƒì§€ ê°„ê²©: {frame_interval}í”„ë ˆì„ë§ˆë‹¤")
    
    # ì¶œë ¥ ì„¤ì •
    output_dir = Path('l_pt_test_results')
    output_dir.mkdir(exist_ok=True)
    
    video_output_path = None
    out_video = None
    if save_output:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        video_output_path = output_dir / f"video_result_{timestamp}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out_video = cv2.VideoWriter(str(video_output_path), fourcc, fps, (width, height))
        print(f"   ì¶œë ¥ íŒŒì¼: {video_output_path}")
    
    # í†µê³„
    frame_count = 0
    detection_count = 0
    total_detections = 0
    detection_times = []
    detections_per_frame = []
    frame_processing_times = []  # ê° í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„ (íƒì§€ + ì‹œê°í™” ë“±)
    
    # ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ì €ì¥ (ë‹¤ìŒ íƒì§€ ì „ê¹Œì§€ í‘œì‹œ)
    last_detections = []
    
    # ë¡œê·¸ íŒŒì¼
    log_file_path = output_dir / f"video_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    log_file = open(log_file_path, 'w', encoding='utf-8')
    
    def log_print(message):
        print(message)
        log_file.write(message + '\n')
        log_file.flush()
    
    log_print(f"ì˜ìƒ í…ŒìŠ¤íŠ¸ ì‹œì‘: {video_path}")
    log_print(f"ì„¤ì •: conf_threshold={conf_threshold}, frame_interval={frame_interval}, process_scale={process_scale}")
    log_print(f"í•´ìƒë„: ì›ë³¸ {width}x{height} â†’ ì²˜ë¦¬ {process_width}x{process_height}")
    log_print("-" * 60)
    
    # ë¹„ë™ê¸°/ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì„¤ì •
    frame_queue = Queue(maxsize=10)  # í”„ë ˆì„ í (ìµœëŒ€ 10ê°œ)
    result_queue = Queue()  # ê²°ê³¼ í
    stop_worker = threading.Event()  # ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ í”Œë˜ê·¸
    
    def detection_worker():
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ íƒì§€ ìˆ˜í–‰í•˜ëŠ” ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while not stop_worker.is_set():
            try:
                # í”„ë ˆì„ íì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
                item = frame_queue.get(timeout=0.1)
                if item is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                frame_num, process_frame_copy, frame_time = item
                
                # í”„ë ˆì„ ê°„ê²©ì— ë”°ë¼ íƒì§€
                should_detect = (frame_num % frame_interval == 0) or (frame_num == 1)
                
                if not should_detect:
                    frame_queue.task_done()
                    continue
                
                # íƒì§€ ìˆ˜í–‰
                detect_start = time.time()
                results = model(process_frame_copy, conf=conf_threshold, verbose=False)
                detect_time = time.time() - detect_start
                
                result = results[0]
                detections = []
                
                if result.boxes is not None and len(result.boxes) > 0:
                    for box in result.boxes:
                        cls = int(box.cls[0])
                        conf = float(box.conf[0])
                        xyxy = box.xyxy[0].cpu().numpy()
                        
                        # ì›ë³¸ í•´ìƒë„ ì¢Œí‘œë¡œ ë³€í™˜
                        if process_scale < 1.0:
                            xyxy = [
                                xyxy[0] * scale_x,
                                xyxy[1] * scale_y,
                                xyxy[2] * scale_x,
                                xyxy[3] * scale_y
                            ]
                        
                        class_name = result.names[cls] if hasattr(result, 'names') else f"Class_{cls}"
                        
                        detections.append({
                            'class': class_name,
                            'confidence': conf,
                            'bbox': xyxy,
                            'class_id': cls
                        })
                
                # ê²°ê³¼ë¥¼ íì— ì €ì¥
                result_queue.put((frame_num, detections, detect_time))
                frame_queue.task_done()
                
            except Empty:
                continue
            except Exception as e:
                log_print(f"ì›Œì»¤ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
                if item:
                    frame_queue.task_done()
    
    # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    worker_thread = threading.Thread(target=detection_worker, daemon=True)
    worker_thread.start()
    log_print("âœ… ë°±ê·¸ë¼ìš´ë“œ íƒì§€ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    print(f"\nâ–¶ï¸  ì˜ìƒ ì²˜ë¦¬ ì‹œì‘... (ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ëª¨ë“œ)")
    start_time = time.time()
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            current_time = frame_count / fps if fps > 0 else 0
            
            # í”„ë ˆì„ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
            frame_start_time = time.time()
            
            # ì²˜ë¦¬ìš© í•´ìƒë„ë¡œ ì¶•ì†Œ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë¥¼ ìœ„í•´)
            if process_scale < 1.0:
                process_frame = cv2.resize(frame, (process_width, process_height), interpolation=cv2.INTER_LINEAR)
            else:
                process_frame = frame
            
            # í”„ë ˆì„ì„ ë°±ê·¸ë¼ìš´ë“œ íì— ì¶”ê°€ (ë…¼ë¸”ë¡œí‚¹)
            try:
                frame_queue.put_nowait((frame_count, process_frame.copy(), current_time))
            except:
                # íê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                try:
                    frame_queue.get_nowait()
                    frame_queue.task_done()
                    frame_queue.put_nowait((frame_count, process_frame.copy(), current_time))
                except:
                    pass
            
            # ê²°ê³¼ íì—ì„œ ìƒˆë¡œìš´ íƒì§€ ê²°ê³¼ í™•ì¸ (ë…¼ë¸”ë¡œí‚¹)
            new_detections = None
            while True:
                try:
                    result_frame_num, result_detections, detect_time = result_queue.get_nowait()
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    if result_detections:
                        detection_times.append(detect_time)
                        frame_detections_count = len(result_detections)
                        detections_per_frame.append(frame_detections_count)
                        detection_count += 1
                        total_detections += len(result_detections)
                        
                        # ë¡œê·¸ ì¶œë ¥
                        log_print(f"í”„ë ˆì„ {result_frame_num} ({result_frame_num / fps if fps > 0 else 0:.2f}ì´ˆ): {frame_detections_count}ê°œ íƒì§€")
                        for i, det in enumerate(result_detections):
                            log_print(f"  [{i+1}] {det['class']}: {det['confidence']:.2%} "
                                     f"at [{int(det['bbox'][0])}, {int(det['bbox'][1])}, "
                                     f"{int(det['bbox'][2])}, {int(det['bbox'][3])}]")
                    
                    # ê°€ì¥ ìµœê·¼ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                    if result_frame_num == frame_count or result_frame_num > len(last_detections) or not last_detections:
                        last_detections = result_detections.copy() if result_detections else []
                        new_detections = result_detections
                    
                except Empty:
                    break
            
            # ê²°ê³¼ ì‹œê°í™” (ìµœì‹  íƒì§€ ê²°ê³¼ ì‚¬ìš©)
            vis_frame = frame.copy()
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            display_detections = last_detections
            for det in display_detections:
                x1, y1, x2, y2 = map(int, det['bbox'])
                
                # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ë¼ë²¨ ì¶”ê°€
                label = f"{det['class']}: {det['confidence']:.1%}"
                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(vis_frame, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), (0, 255, 0), -1)
                cv2.putText(vis_frame, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
            
            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            info_text = f"Frame: {frame_count}/{total_frames} | Time: {current_time:.1f}s"
            if display_detections:
                info_text += f" | Detections: {len(display_detections)}"
            if new_detections is None:
                info_text += " (async)"
            cv2.putText(vis_frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
            if out_video is not None:
                out_video.write(vis_frame)
            
            # í™”ë©´ì— í‘œì‹œ (ëª¨ë“  í”„ë ˆì„ ì¦‰ì‹œ í‘œì‹œ)
            if show_video:
                # í™”ë©´ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                display_width = 1280
                if width > display_width:
                    scale = display_width / width
                    display_height = int(height * scale)
                    display_frame = cv2.resize(vis_frame, (display_width, display_height))
                else:
                    display_frame = vis_frame
                
                cv2.imshow('QR Detection Test (Async)', display_frame)
                
                # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    break
            
            # í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • (í‘œì‹œë§Œ, ì‹¤ì œ íƒì§€ëŠ” ë°±ê·¸ë¼ìš´ë“œ)
            frame_processing_time = time.time() - frame_start_time
            frame_processing_times.append(frame_processing_time)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì‹¤ì‹œê°„ ì²˜ë¦¬ ì†ë„ í¬í•¨)
            if frame_count % 30 == 0:
                progress = (frame_count / total_frames) * 100
                # í˜„ì¬ê¹Œì§€ì˜ í‰ê·  ì²˜ë¦¬ FPS ê³„ì‚°
                elapsed_time = time.time() - start_time
                current_processing_fps = frame_count / elapsed_time if elapsed_time > 0 else 0
                speed_ratio = (current_processing_fps / fps * 100) if fps > 0 else 0
                print(f"   ì§„í–‰: {progress:.1f}% ({frame_count}/{total_frames} í”„ë ˆì„) | "
                      f"ì²˜ë¦¬ ì†ë„: {current_processing_fps:.2f} FPS (ì›ë³¸ {fps:.2f} FPSì˜ {speed_ratio:.1f}%)")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    finally:
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ
        stop_worker.set()
        frame_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸
        worker_thread.join(timeout=2.0)
        log_print("âœ… ë°±ê·¸ë¼ìš´ë“œ íƒì§€ ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ")
        
        # ì •ë¦¬
        total_time = time.time() - start_time
        cap.release()
        if out_video is not None:
            out_video.release()
        if show_video:
            cv2.destroyAllWindows()
        
        # í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ í”„ë ˆì„: {frame_count}")
        print(f"   íƒì§€ëœ í”„ë ˆì„: {detection_count}")
        print(f"   ì´ íƒì§€ ìˆ˜: {total_detections}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ì²˜ë¦¬ ì†ë„ í†µê³„
        if frame_count > 0 and total_time > 0:
            actual_fps = frame_count / total_time
            print(f"\nâš¡ ì²˜ë¦¬ ì†ë„ ë¶„ì„:")
            print(f"   ì›ë³¸ ì˜ìƒ FPS: {fps:.2f}")
            print(f"   ì‹¤ì œ ì²˜ë¦¬ FPS: {actual_fps:.2f}")
            speed_ratio = (actual_fps / fps * 100) if fps > 0 else 0
            print(f"   ì†ë„ ë¹„ìœ¨: {speed_ratio:.1f}% (ì›ë³¸ ëŒ€ë¹„)")
            if actual_fps >= fps:
                print(f"   âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥! (ì›ë³¸ë³´ë‹¤ {actual_fps/fps:.2f}x ë¹ ë¦„)")
            else:
                print(f"   âš ï¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ ë¶ˆê°€ (ì›ë³¸ì˜ {actual_fps/fps:.2f}x ëŠë¦¼)")
        
        if frame_processing_times:
            avg_frame_time = np.mean(frame_processing_times)
            min_frame_time = np.min(frame_processing_times)
            max_frame_time = np.max(frame_processing_times)
            print(f"\nğŸ“ˆ í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„:")
            print(f"   í‰ê· : {avg_frame_time*1000:.2f}ms")
            print(f"   ìµœì†Œ: {min_frame_time*1000:.2f}ms")
            print(f"   ìµœëŒ€: {max_frame_time*1000:.2f}ms")
            if fps > 0:
                target_frame_time = 1.0 / fps
                print(f"   ëª©í‘œ (ì›ë³¸ FPS ê¸°ì¤€): {target_frame_time*1000:.2f}ms")
        
        if detection_times:
            avg_detect_time = np.mean(detection_times)
            print(f"\nğŸ” íƒì§€ ì‹œê°„:")
            print(f"   í‰ê·  íƒì§€ ì‹œê°„: {avg_detect_time*1000:.2f}ms")
        if detections_per_frame:
            avg_detections = np.mean(detections_per_frame)
            print(f"   í”„ë ˆì„ë‹¹ í‰ê·  íƒì§€: {avg_detections:.2f}ê°œ")
        
        log_print("-" * 60)
        log_print(f"ì²˜ë¦¬ ì™„ë£Œ")
        log_print(f"ì´ í”„ë ˆì„: {frame_count}, íƒì§€ í”„ë ˆì„: {detection_count}, ì´ íƒì§€: {total_detections}")
        log_print(f"ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        # ì²˜ë¦¬ ì†ë„ ë¡œê·¸
        if frame_count > 0 and total_time > 0:
            actual_fps = frame_count / total_time
            log_print(f"\nâš¡ ì²˜ë¦¬ ì†ë„ ë¶„ì„:")
            log_print(f"   ì›ë³¸ ì˜ìƒ FPS: {fps:.2f}")
            log_print(f"   ì‹¤ì œ ì²˜ë¦¬ FPS: {actual_fps:.2f}")
            speed_ratio = (actual_fps / fps * 100) if fps > 0 else 0
            log_print(f"   ì†ë„ ë¹„ìœ¨: {speed_ratio:.1f}% (ì›ë³¸ ëŒ€ë¹„)")
            if actual_fps >= fps:
                log_print(f"   âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥! (ì›ë³¸ë³´ë‹¤ {actual_fps/fps:.2f}x ë¹ ë¦„)")
            else:
                log_print(f"   âš ï¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ ë¶ˆê°€ (ì›ë³¸ì˜ {actual_fps/fps:.2f}x ëŠë¦¼)")
        
        if frame_processing_times:
            avg_frame_time = np.mean(frame_processing_times)
            min_frame_time = np.min(frame_processing_times)
            max_frame_time = np.max(frame_processing_times)
            log_print(f"\nğŸ“ˆ í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„:")
            log_print(f"   í‰ê· : {avg_frame_time*1000:.2f}ms")
            log_print(f"   ìµœì†Œ: {min_frame_time*1000:.2f}ms")
            log_print(f"   ìµœëŒ€: {max_frame_time*1000:.2f}ms")
            if fps > 0:
                target_frame_time = 1.0 / fps
                log_print(f"   ëª©í‘œ (ì›ë³¸ FPS ê¸°ì¤€): {target_frame_time*1000:.2f}ms")
        
        log_file.close()
        
        if video_output_path:
            print(f"   ğŸ’¾ ê²°ê³¼ ì˜ìƒ ì €ì¥: {video_output_path}")
            print(f"   ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file_path}")
    
    return {
        'total_frames': frame_count,
        'detection_frames': detection_count,
        'total_detections': total_detections,
        'output_video': video_output_path,
        'log_file': log_file_path
    }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    model_path = 'l.pt'
    video_path = r'C:\Users\Administrator\qr_sh\data\video\sample_video3-1.mp4'
    
    # 1. ëª¨ë¸ ì •ë³´ í™•ì¸
    model, model_type = test_model_info(model_path)
    
    if model is None:
        print("\nâŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. YOLO ëª¨ë¸ì¸ ê²½ìš° í…ŒìŠ¤íŠ¸
    if model_type == 'yolo':
        print("\n" + "=" * 60)
        print("ğŸ¬ ì˜ìƒ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        print("=" * 60)
        
        # ì˜ìƒ í…ŒìŠ¤íŠ¸
        if os.path.exists(video_path):
            print(f"\nğŸ“¹ ì˜ìƒ íŒŒì¼: {video_path}")
            
            # ì‚¬ìš©ì ì„¤ì •
            conf_threshold = 0.25  # ì‹ ë¢°ë„ ì„ê³„ê°’
            frame_interval = 2     # 2í”„ë ˆì„ë§ˆë‹¤ íƒì§€ (ì†ë„ í–¥ìƒ)
            process_scale = 1.0    # ì²˜ë¦¬ í•´ìƒë„ ìŠ¤ì¼€ì¼ (1.0=ì›ë³¸, 0.5=50%, 0.25=25%)
            show_video = True      # í™”ë©´ì— í‘œì‹œ ì—¬ë¶€
            save_output = True     # ê²°ê³¼ ì˜ìƒ ì €ì¥ ì—¬ë¶€
            
            result = test_video_detection(
                model, 
                video_path, 
                conf_threshold=conf_threshold,
                frame_interval=frame_interval,
                show_video=show_video,
                save_output=save_output,
                process_scale=process_scale
            )
            
            if result:
                print(f"\nâœ… ì˜ìƒ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        else:
            print(f"\nâŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            print(f"\nğŸ“ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ë¡œ ì „í™˜...")
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
            test_images_dir = Path('data/250723_test')
            
            if test_images_dir.exists():
                test_images = list(test_images_dir.glob('*.jpg'))[:5]
                
                if len(test_images) > 0:
                    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ê°œ")
                    
                    output_dir = Path('l_pt_test_results')
                    output_dir.mkdir(exist_ok=True)
                    
                    for i, img_path in enumerate(test_images):
                        print(f"\n{'='*60}")
                        result_data = test_yolo_detection(model, str(img_path))
                        
                        if result_data:
                            save_path = output_dir / f"result_{i+1}_{img_path.stem}.png"
                            visualize_results(result_data['image'], result_data['detections'], 
                                            str(save_path))
                    
                    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_images_dir}")
    
    elif model_type == 'pytorch':
        print("\nâš ï¸ ì¼ë°˜ PyTorch ëª¨ë¸ì…ë‹ˆë‹¤. YOLO ì „ìš© í…ŒìŠ¤íŠ¸ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == '__main__':
    main()

