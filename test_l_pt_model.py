"""
l.pt ëª¨ë¸ íŒŒì¼ ë™ì‘ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
ì˜ìƒ ë° ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì§€ì›
"""

import cv2
import numpy as np
import os
from pathlib import Path
import torch
import time
import datetime

# Ultralytics YOLO ëª¨ë¸ ë¡œë“œ ì‹œë„
try:
    from ultralytics import YOLO
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("âš ï¸ ultralyticsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í‘œì‹œìš© ì„¤ì •
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

def test_model_info(model_path='l.pt'):
    """ëª¨ë¸ ì •ë³´ í™•ì¸"""
    print("=" * 60)
    print(f"ğŸ“¦ ëª¨ë¸ íŒŒì¼: {model_path}")
    print("=" * 60)
    
    if not os.path.exists(model_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    
    file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
    print(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
    
    # Ultralytics YOLO ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„
    if ULTRALYTICS_AVAILABLE:
        try:
            print("\nğŸ” Ultralytics YOLO ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„...")
            model = YOLO(model_path)
            
            print(f"âœ… ëª¨ë¸ íƒ€ì…: YOLO (Ultralytics)")
            print(f"ğŸ“‹ ëª¨ë¸ ì •ë³´:")
            print(f"   - Task: {model.task if hasattr(model, 'task') else 'Unknown'}")
            print(f"   - Classes: {len(model.names) if hasattr(model, 'names') else 'Unknown'}")
            
            if hasattr(model, 'names'):
                print(f"   - í´ë˜ìŠ¤ ëª©ë¡:")
                for idx, name in model.names.items():
                    print(f"     [{idx}] {name}")
            
            if hasattr(model, 'model'):
                model_info = model.model
                print(f"   - ëª¨ë¸ êµ¬ì¡°: {type(model_info).__name__}")
            
            return model, 'yolo'
        except Exception as e:
            print(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ì¼ë°˜ PyTorch ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„
    try:
        print("\nğŸ” PyTorch ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„...")
        checkpoint = torch.load(model_path, map_location='cpu')
        
        print(f"âœ… ëª¨ë¸ íƒ€ì…: PyTorch")
        print(f"ğŸ“‹ ì²´í¬í¬ì¸íŠ¸ í‚¤:")
        for key in checkpoint.keys():
            if isinstance(checkpoint[key], dict):
                print(f"   - {key}: (dict with {len(checkpoint[key])} keys)")
                if len(checkpoint[key].keys()) < 10:
                    for subkey in checkpoint[key].keys():
                        print(f"     - {subkey}: {type(checkpoint[subkey]).__name__}")
            elif isinstance(checkpoint[key], (list, tuple)):
                print(f"   - {key}: ({type(checkpoint[key]).__name__} with {len(checkpoint[key])} items)")
            else:
                print(f"   - {key}: {type(checkpoint[key]).__name__}")
        
        return checkpoint, 'pytorch'
    except Exception as e:
        print(f"âŒ PyTorch ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def test_yolo_detection(model, image_path, conf_threshold=0.25):
    """YOLO ëª¨ë¸ë¡œ ì´ë¯¸ì§€ íƒì§€ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ–¼ï¸  ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸: {os.path.basename(image_path)}")
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = cv2.imread(image_path)
    if image is None:
        print(f"âŒ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        return None
    
    print(f"   ì´ë¯¸ì§€ í¬ê¸°: {image.shape[1]}x{image.shape[0]}")
    
    # íƒì§€ ì‹¤í–‰
    try:
        results = model(image, conf=conf_threshold, verbose=False)
        result = results[0]
        
        print(f"   íƒì§€ëœ ê°ì²´ ìˆ˜: {len(result.boxes) if result.boxes is not None else 0}")
        
        detections = []
        if result.boxes is not None and len(result.boxes) > 0:
            for i, box in enumerate(result.boxes):
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                xyxy = box.xyxy[0].cpu().numpy()
                
                class_name = result.names[cls] if hasattr(result, 'names') else f"Class_{cls}"
                
                detections.append({
                    'class': class_name,
                    'confidence': conf,
                    'bbox': xyxy,
                    'class_id': cls
                })
                
                print(f"   [{i+1}] {class_name}: {conf:.2%} at [{int(xyxy[0])}, {int(xyxy[1])}, {int(xyxy[2])}, {int(xyxy[3])}]")
        
        return {
            'image': image,
            'detections': detections,
            'result': result
        }
    except Exception as e:
        print(f"âŒ íƒì§€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def visualize_results(image, detections, save_path=None):
    """íƒì§€ ê²°ê³¼ ì‹œê°í™”"""
    if detections is None or len(detections) == 0:
        print("   âš ï¸ íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì´ë¯¸ì§€ ë³µì‚¬
    vis_image = image.copy()
    
    # BGR to RGB ë³€í™˜
    vis_image_rgb = cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB)
    
    # Matplotlibìœ¼ë¡œ ì‹œê°í™”
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax.imshow(vis_image_rgb)
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    for det in detections:
        bbox = det['bbox']
        x1, y1, x2, y2 = map(int, bbox)
        
        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        rect = Rectangle((x1, y1), x2-x1, y2-y1, 
                        linewidth=2, edgecolor='red', facecolor='none')
        ax.add_patch(rect)
        
        # ë¼ë²¨ ì¶”ê°€
        label = f"{det['class']}: {det['confidence']:.2%}"
        ax.text(x1, y1-5, label, color='red', fontsize=10, 
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    ax.axis('off')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥: {save_path}")
    
    plt.show()

def test_video_detection(model, video_path, conf_threshold=0.25, 
                        frame_interval=30, show_video=True, save_output=True):
    """YOLO ëª¨ë¸ë¡œ ì˜ìƒ íƒì§€ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¬ ì˜ìƒ í…ŒìŠ¤íŠ¸: {os.path.basename(video_path)}")
    print("=" * 60)
    
    # ë¹„ë””ì˜¤ íŒŒì¼ í™•ì¸
    if not os.path.exists(video_path):
        print(f"âŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return None
    
    # ë¹„ë””ì˜¤ ìº¡ì²˜
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return None
    
    # ë¹„ë””ì˜¤ ì •ë³´
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    duration = total_frames / fps if fps > 0 else 0
    
    print(f"ğŸ“¹ ì˜ìƒ ì •ë³´:")
    print(f"   í•´ìƒë„: {width}x{height}")
    print(f"   FPS: {fps:.2f}")
    print(f"   ì´ í”„ë ˆì„: {total_frames}")
    print(f"   ê¸¸ì´: {duration:.2f}ì´ˆ")
    print(f"   íƒì§€ ê°„ê²©: {frame_interval}í”„ë ˆì„ë§ˆë‹¤")
    
    # ì¶œë ¥ ì„¤ì •
    output_dir = Path('l_pt_test_results')
    output_dir.mkdir(exist_ok=True)
    
    video_output_path = None
    out_video = None
    if save_output:
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        video_output_path = output_dir / f"video_result_{timestamp}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out_video = cv2.VideoWriter(str(video_output_path), fourcc, fps, (width, height))
        print(f"   ì¶œë ¥ íŒŒì¼: {video_output_path}")
    
    # í†µê³„
    frame_count = 0
    detection_count = 0
    total_detections = 0
    detection_times = []
    detections_per_frame = []
    
    # ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ì €ì¥ (ë‹¤ìŒ íƒì§€ ì „ê¹Œì§€ í‘œì‹œ)
    last_detections = []
    
    # ë¡œê·¸ íŒŒì¼
    log_file_path = output_dir / f"video_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    log_file = open(log_file_path, 'w', encoding='utf-8')
    
    def log_print(message):
        print(message)
        log_file.write(message + '\n')
        log_file.flush()
    
    log_print(f"ì˜ìƒ í…ŒìŠ¤íŠ¸ ì‹œì‘: {video_path}")
    log_print(f"ì„¤ì •: conf_threshold={conf_threshold}, frame_interval={frame_interval}")
    log_print("-" * 60)
    
    print(f"\nâ–¶ï¸  ì˜ìƒ ì²˜ë¦¬ ì‹œì‘...")
    start_time = time.time()
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            current_time = frame_count / fps if fps > 0 else 0
            
            # í”„ë ˆì„ ê°„ê²©ì— ë”°ë¼ íƒì§€
            should_detect = (frame_count % frame_interval == 0) or (frame_count == 1)
            
            detections = []
            if should_detect:
                # íƒì§€ ìˆ˜í–‰
                detect_start = time.time()
                results = model(frame, conf=conf_threshold, verbose=False)
                detect_time = time.time() - detect_start
                detection_times.append(detect_time)
                
                result = results[0]
                
                if result.boxes is not None and len(result.boxes) > 0:
                    frame_detections = 0
                    for box in result.boxes:
                        cls = int(box.cls[0])
                        conf = float(box.conf[0])
                        xyxy = box.xyxy[0].cpu().numpy()
                        
                        class_name = result.names[cls] if hasattr(result, 'names') else f"Class_{cls}"
                        
                        detections.append({
                            'class': class_name,
                            'confidence': conf,
                            'bbox': xyxy,
                            'class_id': cls
                        })
                        
                        frame_detections += 1
                        total_detections += 1
                    
                    detection_count += 1
                    detections_per_frame.append(frame_detections)
                    
                    # ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ì—…ë°ì´íŠ¸
                    last_detections = detections.copy()
                    
                    log_print(f"í”„ë ˆì„ {frame_count} ({current_time:.2f}ì´ˆ): {frame_detections}ê°œ íƒì§€")
                    for i, det in enumerate(detections):
                        log_print(f"  [{i+1}] {det['class']}: {det['confidence']:.2%} "
                                 f"at [{int(det['bbox'][0])}, {int(det['bbox'][1])}, "
                                 f"{int(det['bbox'][2])}, {int(det['bbox'][3])}]")
            
            # ê²°ê³¼ ì‹œê°í™” (ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ì‚¬ìš©)
            vis_frame = frame.copy()
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            display_detections = last_detections if not should_detect or len(detections) == 0 else detections
            for det in display_detections:
                x1, y1, x2, y2 = map(int, det['bbox'])
                
                # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.rectangle(vis_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # ë¼ë²¨ ì¶”ê°€
                label = f"{det['class']}: {det['confidence']:.1%}"
                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(vis_frame, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), (0, 255, 0), -1)
                cv2.putText(vis_frame, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
            
            # í”„ë ˆì„ ì •ë³´ í‘œì‹œ
            info_text = f"Frame: {frame_count}/{total_frames} | Time: {current_time:.1f}s"
            if display_detections:
                info_text += f" | Detections: {len(display_detections)}"
            if not should_detect and display_detections:
                info_text += " (cached)"
            cv2.putText(vis_frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
            if out_video is not None:
                out_video.write(vis_frame)
            
            # í™”ë©´ì— í‘œì‹œ
            if show_video:
                # í™”ë©´ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
                display_width = 1280
                if width > display_width:
                    scale = display_width / width
                    display_height = int(height * scale)
                    display_frame = cv2.resize(vis_frame, (display_width, display_height))
                else:
                    display_frame = vis_frame
                
                cv2.imshow('QR Detection Test', display_frame)
                
                # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    break
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if frame_count % (frame_interval * 10) == 0:
                progress = (frame_count / total_frames) * 100
                print(f"   ì§„í–‰: {progress:.1f}% ({frame_count}/{total_frames} í”„ë ˆì„)")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    finally:
        # ì •ë¦¬
        total_time = time.time() - start_time
        cap.release()
        if out_video is not None:
            out_video.release()
        if show_video:
            cv2.destroyAllWindows()
        
        # í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   ì´ í”„ë ˆì„: {frame_count}")
        print(f"   íƒì§€ëœ í”„ë ˆì„: {detection_count}")
        print(f"   ì´ íƒì§€ ìˆ˜: {total_detections}")
        print(f"   ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        if detection_times:
            avg_detect_time = np.mean(detection_times)
            print(f"   í‰ê·  íƒì§€ ì‹œê°„: {avg_detect_time*1000:.2f}ms")
        if detections_per_frame:
            avg_detections = np.mean(detections_per_frame)
            print(f"   í”„ë ˆì„ë‹¹ í‰ê·  íƒì§€: {avg_detections:.2f}ê°œ")
        
        log_print("-" * 60)
        log_print(f"ì²˜ë¦¬ ì™„ë£Œ")
        log_print(f"ì´ í”„ë ˆì„: {frame_count}, íƒì§€ í”„ë ˆì„: {detection_count}, ì´ íƒì§€: {total_detections}")
        log_print(f"ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        log_file.close()
        
        if video_output_path:
            print(f"   ğŸ’¾ ê²°ê³¼ ì˜ìƒ ì €ì¥: {video_output_path}")
            print(f"   ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file_path}")
    
    return {
        'total_frames': frame_count,
        'detection_frames': detection_count,
        'total_detections': total_detections,
        'output_video': video_output_path,
        'log_file': log_file_path
    }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    model_path = 'l.pt'
    video_path = r'C:\Users\Administrator\qr_sh\data\video\sample_video3-1.mp4'
    
    # 1. ëª¨ë¸ ì •ë³´ í™•ì¸
    model, model_type = test_model_info(model_path)
    
    if model is None:
        print("\nâŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. YOLO ëª¨ë¸ì¸ ê²½ìš° í…ŒìŠ¤íŠ¸
    if model_type == 'yolo':
        print("\n" + "=" * 60)
        print("ğŸ¬ ì˜ìƒ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        print("=" * 60)
        
        # ì˜ìƒ í…ŒìŠ¤íŠ¸
        if os.path.exists(video_path):
            print(f"\nğŸ“¹ ì˜ìƒ íŒŒì¼: {video_path}")
            
            # ì‚¬ìš©ì ì„¤ì •
            conf_threshold = 0.25  # ì‹ ë¢°ë„ ì„ê³„ê°’
            frame_interval = 30    # Ní”„ë ˆì„ë§ˆë‹¤ íƒì§€ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
            show_video = True      # í™”ë©´ì— í‘œì‹œ ì—¬ë¶€
            save_output = True     # ê²°ê³¼ ì˜ìƒ ì €ì¥ ì—¬ë¶€
            
            result = test_video_detection(
                model, 
                video_path, 
                conf_threshold=conf_threshold,
                frame_interval=frame_interval,
                show_video=show_video,
                save_output=save_output
            )
            
            if result:
                print(f"\nâœ… ì˜ìƒ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        else:
            print(f"\nâŒ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            print(f"\nğŸ“ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ë¡œ ì „í™˜...")
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
            test_images_dir = Path('data/250723_test')
            
            if test_images_dir.exists():
                test_images = list(test_images_dir.glob('*.jpg'))[:5]
                
                if len(test_images) > 0:
                    print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(test_images)}ê°œ")
                    
                    output_dir = Path('l_pt_test_results')
                    output_dir.mkdir(exist_ok=True)
                    
                    for i, img_path in enumerate(test_images):
                        print(f"\n{'='*60}")
                        result_data = test_yolo_detection(model, str(img_path))
                        
                        if result_data:
                            save_path = output_dir / f"result_{i+1}_{img_path.stem}.png"
                            visualize_results(result_data['image'], result_data['detections'], 
                                            str(save_path))
                    
                    print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    print("âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_images_dir}")
    
    elif model_type == 'pytorch':
        print("\nâš ï¸ ì¼ë°˜ PyTorch ëª¨ë¸ì…ë‹ˆë‹¤. YOLO ì „ìš© í…ŒìŠ¤íŠ¸ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ë ¤ë©´ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == '__main__':
    main()

